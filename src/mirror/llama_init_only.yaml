class_path: transformers.LlamaConfig
init_args:
  vocab_size: 128256
  hidden_size: 256
  intermediate_size: 1024
  num_hidden_layers: 4
  num_attention_heads: 8
  num_key_value_heads: 8
